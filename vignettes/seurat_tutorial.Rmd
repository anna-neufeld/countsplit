---
title: "Tutorial: count splitting with seurat" 
output: rmarkdown::html_vignette
bibliography: latent.bib
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Before using this tutorial, we recommend that you read through our ``introduction to count splitting" tutorial to understand our method in a simple example with simulated data. 

The purpose of this tutorial is to reproduce the analysis from the Seurat [clustering](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html) tutorial while using `countsplit` to avoid the double dipping that was inherent in that tutorial. The analysis mirrors the analysis in the June, 2022 version of the Seurat tutorial. In the event that the Seurat tutorial at the link above gets modified, we have also reproduced the relevant analyses below for reference. 

We use the same data and carry out the same processes as in the Seurat tutorial, but we highlight the steps that should be performed on the training set as opposed to on the test set. We also point out some steps in the pipeline where count splitting could potentially cause confusion. For more information on the Seurat methodology, see the [website](https://satijalab.org/seurat/) or papers such as @seurat1, @seurat2, @seurat3. 

# Install Seurat and countsplit

If you don't already have ``Seurat``, you will need to run:

```{r, eval=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("Seurat")
```

Make sure that ``remotes`` is installed by running ``install.packages("remotes")``, then type

```{r}
remotes::install_github("anna-neufeld/countsplit")
```

# Loading the data

We use the same dataset as in the Seurat [clustering](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html) vignette. For convenience, we included the `pbmc` dataset raw counts (obtained from 10x genomics) in the `countsplit` package. We just need to load the packages and load the count matrix. If any of the packages besides `Seurat` and `countsplit` are not installed, they can be installed from `cran` with `install.packages()`. 

```{r}
library(Seurat)
library(countsplit)
library(ggplot2)
library(patchwork)
library(mclust)
data(pbmc.counts, package="countsplit")
```

Seurat objects cannot handle gene names that have underscores in them. To avoid issues later in the tutorial (where we will need to use gene names to map between the training and test sets), we run the following code to replace all underscores with dashes in the gene names of the raw counts matrix. 

```{r}
rownames(pbmc.counts) <- sapply(rownames(pbmc.counts), function(u) stringr::str_replace_all(u, "_","-"))
```

# Applying count splitting and creating a Seurat object

We now count split to obtain two raw count matrices. This is the only place in this tutorial where we actually use a function from the `countsplit` package. 

```{r}
set.seed(1)
split <- countsplit(pbmc.counts, epsilon=0.5)
Xtrain <- split$train
Xtest <- split$test
```

We now must store the training matrix in a Seurat object so that we can apply the preprocessing steps from the Seurat clustering tutorial. As recommended by Seurat, this code will remove any genes that were not expressed in at least `3` cells and will remove any cells that did not have at least `200` expressed genes. 

```{r}
pbmc.train <- CreateSeuratObject(counts = Xtrain, min.cells = 3, min.features = 200)
```

For the sake of comparing our analysis to the one in the Seurat tutorial, we also create a `pbmc` object that contains the full expression matrix, as opposed to the training set. Any time we apply operations to `pbmc.train`, we will apply the same operations to `pbmc` for the sake of comparison. 

```{r}
pbmc <- CreateSeuratObject(counts = pbmc.counts, min.cells = 3, min.features = 200)
```

The Seurat tutorial then recommends further subsetting the cells to exclude cells that have unique feature counts over 2,500 or less than 200, and to exclude cells that have >5% mitochondrial counts

```{r}
# Apply to training object
pbmc.train[["percent.mt"]] <- PercentageFeatureSet(pbmc.train, pattern = "^MT-")
pbmc.train <- subset(pbmc.train, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)

# Apply to full object
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
```

We note that now the dimensions of `Xtrain` and `Xtest` do not match up with the dimensions of our new Seurat object.

```{r}
dim(Xtrain)
dim(Xtest)
dim(pbmc.train)
```

To avoid any confusion later, we create `Xtestsubset`, which contains the same genes and the same cells as `pbmc.train`. 

```{r}
rows <- rownames(pbmc.train)
cols <- colnames(pbmc.train)
Xtestsubset <- Xtest[rows,cols]
dim(Xtestsubset)
```

We also note that `pbmc.train` and `pbmc` do not have the same dimensions, as the original requirement about the number of non-zero counts for each gene/cell was applied separately to the full data and the training set. Later, when it is needed to make comparisons, we will subset `pbmc` appropriately. 

```{r}
dim(pbmc.train)
dim(pbmc)
```

# Preprocessing workflow

For our count splitting analysis, all steps in the preprocessing workflow are performed on `pbmc.train`. Importantly, the test set is left untouched throughout this section. We perform the workflow on `pbmc` in paralell for the sake of comparison. 

We take this time to point out some intricacies of the ``Seurat`` object that could become confusing in future analyses. A ``Seurat`` object has three assays: ``counts``, ``data``, and ``scale.data``. At this point in the analysis, ``data`` and ``counts`` both store the raw counts, and ``scale.data`` is empty. 

```{r}
all.equal(GetAssayData(pbmc.train, "counts"), GetAssayData(pbmc.train, "data"))
```

```{r}
GetAssayData(pbmc.train, "scale.data")
```

These assays will change as we run further preprocessing steps, and this will be important to keep in mind. We next normalize and compute the set of highly variable features, as in the Seurat tutorial. Note that normalizing changes the ``data`` assay of `pbmc.train` such that it stores normalized data, rather than counts. 

```{r}
pbmc.train <- NormalizeData(pbmc.train)
all.equal(GetAssayData(pbmc.train, "counts"), GetAssayData(pbmc.train, "data"))

# Apply to pbmc, for the sake of comparison. 
pbmc <- NormalizeData(pbmc)
```

Computing the set of highly variable features does not alter the dimension of the dataset. All features are retained, but these highly variable features are the ones that will be used downstream during dimension reduction. 

```{r}
dim(pbmc.train)
pbmc.train <- FindVariableFeatures(pbmc.train, selection.method = "vst", nfeatures = 2000)
dim(pbmc.train)

# Apply to pbmc, for the sake of comparison. 
pbmc  <- FindVariableFeatures(pbmc, selection.method = "vst", nfeatures = 2000)
```

At this point in the Seurat tutorial, they make a plot with the top 10 variable features labeled. Here, we reproduce this plot on the entire dataset and show that the plot obtained using only the training set is quite similar. 

```{r}
top10 <- head(VariableFeatures(pbmc), 10)
plot1 <- VariableFeaturePlot(pbmc) + ggtitle("pbmc")
plot2 <- LabelPoints(plot = plot1, points = top10)
top10.train <- head(VariableFeatures(pbmc.train), 10)
plot1.train <- VariableFeaturePlot(pbmc.train) + ggtitle("pbmc.train")
plot2.train <- LabelPoints(plot = plot1.train, points = top10.train)


plot2 + plot2.train & guides(col="none")


```

The analysis on `pbmc` and the analysis on `pbmc.train` identify similar sets of genes as the top 10 most highly variable genes. This is comforting, as it shows that the training set is retaining a lot of info compared to the full dataset. The overlapping genes are: ``PPBP, LYZ, FTL, S100A9, S100A8, GNLY, FTH1, IGLL5, PF4``. The only difference is that on the training set we selected ``HLA-DRA`` instead of ``GNG11``.   

We continue following the preprocessing steps from the Seurat tutorial on both `pbmc` and `pbmc.train`. The next step is to scale the data and to compute principal components. Below, we compare loading plots for the first two principal components for `pbmc` and `pbmc.train`. We note that ``ScaleData`` finally fills in the ``scale.data`` assay in the object, and some downstream functions will access this assay. 

```{r,out.width="100%"}
all.genes <- rownames(pbmc)
pbmc <- ScaleData(pbmc,features = all.genes)
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
p1 <- VizDimLoadings(pbmc, dims = 1, reduction = "pca")+theme(axis.text = element_text(size=10))+ggtitle("pbmc")
p2 <- VizDimLoadings(pbmc, dims = 2, reduction = "pca")+theme(axis.text = element_text(size=10))

all.genes.train <- rownames(pbmc.train)
pbmc.train <- ScaleData(pbmc.train,features = all.genes.train)
pbmc.train <- RunPCA(pbmc.train, features = VariableFeatures(object = pbmc.train))
p1.train <- VizDimLoadings(pbmc.train, dims = 1, reduction = "pca")+theme(axis.text = element_text(size=10))+ggtitle("pbmc.train")
p2.train <- VizDimLoadings(pbmc.train, dims = 2, reduction = "pca")+theme(axis.text = element_text(size=10))

p1+p1.train+p2+p2.train+plot_layout(nrow=2, ncol=2)
```

We can see that we obtain very similar principal components on the training set to those obtained on the full data (up to a sign flip). On both datasets, PC1 is dominated by the gene `MALAT1`, and genes like `CST3, TYROBF` are of lesser importance.  The second principal component is dominated by `CD79A` and `HLA-D0A1` and `HLA-DOB1`, wich `NKG7` having lesser importance. 

Now that we have convinced ourselves that the training set is retaining a lot of the signal of the full dataset (and is thus able to estimate very similar principal components), we move on to clustering. We skip the next several visualizations and steps of the Seurat tutorial for brevity. We will return to the PC heatmaps at the end of this document. As in the Seurat tutorial, we retain 10 principal components for clustering. 

```{r}
pbmc <- FindNeighbors(pbmc, dims = 1:10)
pbmc <- FindClusters(pbmc, resolution=0.5)
pbmc.train <- FindNeighbors(pbmc.train, dims = 1:10)
pbmc.train <- FindClusters(pbmc.train, resolution=0.5)
```

Finally, we visualize the clusters that we have computed. 

```{r,}
pbmc.train <- RunUMAP(pbmc.train, dims = 1:10)
DimPlot(pbmc.train, reduction = "umap")
```

We want to know how similar these clusters are to the ones computed on the full data. Looking at two UMAP plots could be potentially misleading, as the UMAP dimensions on the full dataset are \emph{different} than those on the training set. Instead, we look at the adjusted Rand index (@hubert1985comparing) between the clustering on the full dataset and the clustering on the training set. As a reminder, the training set has slightly fewer cells in it due to the preprocessing pipeline. We will look at the adjusted Rand index that takes into account only the cells in the training set.

```{r}
clusters.train <- Idents(pbmc.train)
clusters.full.subset <- Idents(pbmc)[colnames(pbmc.train)]
length(clusters.train)
length(clusters.full.subset)
adjustedRandIndex(clusters.train, clusters.full.subset)
```

```{r}
table(clusters.train, clusters.full.subset)
```

# Finding differentially expressed features manually

Now that we computed clusters from the training set, it is time to look for differentially expressed genes across the clusters. The "safest" way to perform this analysis is to extract the cluster labels from ``pbmc.train`` and write our own analysis functions to see how the columns of ``Xtestsubset`` (created above) vary across these clusters. This approach is the safest because we know for sure that the clusters were obtained using only the training data and that the analysis uses only the testing data.

First, we extract the clusters and verify that we have a cluster for every cell in ``Xtestsubset``. 

```{r}
clusters.train <- Idents(pbmc.train)
table(clusters.train)
length(clusters.train)
NCOL(Xtestsubset)
```

Next, we note that the ``FindMarkers`` function in Seurat (which is used for differential expression testing) does a wilcox test by default. The first example of differential expression testing in their tutorial compares cluster 2 against all other clusters. From visual inspection, it seems that their cluster 2 matches up well with our cluster 2 (this was just good luck; several other clusters have received different labels). Therefore, we try to reproduce their cluster 2 analysis by hand.

Note that the ``FindMarkers`` function acts on the log-Normalized data, and so to reproduce this analysis by hand we must log and normalize the test data. 

```{r}
sf <- colSums(Xtestsubset)
Xtestsubset_norm <- t(apply(Xtestsubset, 1, function(u) u/sf))
Xtestsubset_lognorm <- log(Xtestsubset_norm +1)
```

For computational simplicity, the Seurat tutorial only tests a subset of genes for differential expression. For simplicity, here we test all of the genes. 

```{r}
cluster2 <- clusters.train==2
pvals2 <- apply(Xtestsubset_lognorm, 1, function(u) wilcox.test(u~cluster2)$p.value)
head(sort(pvals2))
```

We identify ``LTB``, ``IL32``, ``CD3D``, ``IL7R``, and ``LDHB`` as the top markers of cluster 2, which is exactly the same as in the Seurat tutorial (although the order is slightly different and the p0values are slightly different). 

# Finding differentially expressed features with Seurat

It would be nice to be able to make a ``Seurat`` object containing the training set clusters but the test set counts. This would allow us to use some of Seurat's nice visualization features for differential expression testing. We note that care should be taken in this section, as we are mixing-and-matching pieces of Seurat objects and Seurat functions not explicitly mentioned in this tutorial may have unexpected behavior. 

We first let `pbmc.test` equal `pbmc.train`. This ensures that our test set object gets the same cluster information and UMAP information as the training set, such that we can still visualize the training set clusters. We then update the `counts` assay to store the test set counts. Unfortunately, once we do this, the `data` and `scale.data` assays are not automatically updated to store the normalized or scaled counts. We run normalize and scale functions to remedy this.

```{r}
pbmc.test <- pbmc.train
pbmc.test <- SetAssayData(object = pbmc.test, slot = "counts", new.data = Xtestsubset)
pbmc.test <- NormalizeData(pbmc.test)
pbmc.test <- ScaleData(pbmc.test, features = all.genes)
```

We first verify that the Seurat ``FindMarkers`` function returns the same information as the manual differential expression test above. It does! 

```{r}
cluster2.markers <- FindMarkers(pbmc.test, ident.1 = 2, min.pct = 0)
head(sort(pvals2), n=10)
head(cluster2.markers, n = 10)
```

The main reason it is useful to store the test matrix in a ``Seurat`` object is that it lets us use many cool visualization features. 

Consider the following sets of heatmaps. 

This first set of heatmaps is computed using the training counts. All principal components are computed on the training set. Each panel shows a principal component. 500 randomly selected cells are ordered along the principal component on the X-axis. The genes with the top 5 highest loadings are shown on the Y-axis. While the association between the genes and the principal components clearly decreases as we move from PC1 to PC15, PC15 still clearly shows association between the top 5 genes and the PC. This association is due to the fact that the training data itself was used to construct the PCs-- and so there will always be some genes that appear to be associated with the PC. 
```{r}
DimHeatmap(pbmc.train, dims = 1:15, cells = 500, balanced = TRUE)
```

Now instead we still show principal components computed on the training set, but the expression count values inside of the heat map are now test set counts. For the first 10 or so PCs, the association between test set counts and the PC seems almost as strong as the association between the training set counts and the PCs. This suggests that these PCs are measuring true signal in the data. On the other hand, consider the PCs 11-15. The patterns seen in the training set essentially disappear in the top genes plotted for the test set. This suggests that any association seen in the initial heatmaps was simply due to overfitting; these PCs do not measure anything useful or interesting. 
```{r}
DimHeatmap(pbmc.test, dims = 1:15, cells = 500, balanced = TRUE)
```

This matches insights from the Seurat tutorial (insights obtained using Jackstraw), where they suggested that somewhere between 7-12 PCs would be the right number to keep, and they chose to keep 10.
These heatmaps show the utility of count splitting in helping distinguish signal from noise.


# References