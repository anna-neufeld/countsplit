---
title: "Tutorial: differential expression analysis on single cell RNA-seq data" 
output: rmarkdown::html_vignette
bibliography: latent.bib
vignette: >
  %\VignetteIndexEntry{"Intro Tutorial: differential expression analysis on single cell RNA-seq data"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
```

In this tutorial, we demonstrate how to use the ``countsplit`` package to perform inference after latent variable selection for scRNA-seq data. In this tutorial, we work with simple simulated data and use only the ``countsplit`` package. In the other tutorials on this site, we show how the ``countsplit`` package can be integrated into pipelines with other scRNA-seq packages such as ``scran``, ``monocle3`` and ``seurat``
.
We start by loading the packages we will be working with. Make sure that ``remotes`` is installed by running ``install.packages("remotes")``, then type

```{r,eval=FALSE}
remotes::install_github("anna-neufeld/countsplit")
```

```{r, message=FALSE}
library(countsplit)
```

# Simple example on simulated data 

To get comfortable with the ``countsplit`` package, we start by working with simple simulated datasets. For these analyses, we first cluster the cells using k-means clustering with k=2, and then we test for differential expression using Poisson generalized linear models (GLMs). These simple choices keep us from needing external scRNA-seq packages. 

## Simulated data with no true signal. 

First suppose that we have $n=1000$ cells and $p=200$ genes. Suppose that every count $\textbf{X}_{ij}$ is drawn from a $\text{Poisson}(5)$ distribution. We first generate this data. 

```{r}
set.seed(1)
n <- 1000
p <- 200
X <- matrix(rpois(n*p, lambda=5), nrow=n)
```

Suppose we are interested in studying differential expression across two clusters, obtained with k-means clustering. First we can see that the naive method does not control the Type 1 error rate. Here is an example of what happens when we cluster the data and then test for differential expression using the naive method:

```{r}
clusters.full <- kmeans(log(X+1), centers=2)$cluster
results.naive <- t(apply(X, 2, function(u) summary(glm(u~clusters.full, family="poisson"))$coefficients[2,]))
head(results.naive)
```

The first line runs k-means with $k=2$ on the logged data (with a pseudocount of 1) and saves the cluster assignments for each cell in the dataset as``clusters.full``. For every column $X_j$ in $X$, the second line fits a Poisson GLM of $X_j$ on ``clusters.full`` and saves the summary of the slope coefficient in ``results.naive``. As shown in the output, we have saved a slope coefficient estimate, a standard error, a z-value, and a p-value for every gene in the dataset. 

Even in these first 6 rows of results, we can see that the naive method assigns small p-values to many genes, despite the fact that no genes are truly differentially expressed in this data. We can make a uniform QQ-plot of the p-values for the naive method to see that they are not uniformly distributed and thus do not control the Type 1 error. The p-values are stored in the 4th column of the ``results.naive`` matrix. 

```{r}
library(ggplot2)
ggplot(data=NULL, aes(sample=results.naive[,4]))+geom_qq(distribution="qunif")+geom_abline(col="red")
```

We now address the issue using count splitting. The key steps are (1) running the ``countsplit`` function to get ``Xtrain`` and ``Xtest`` and then (2) running the analysis step with ``Xtest`` as the response and clusters obtained from ``Xtrain`` as the latent variable. The ``countsplit`` function returns a list, which we call ``split`` here, which contains the training set and the test set.

```{r}
set.seed(2)
split <- countsplit(X, epsilon=0.5)
names(split)
Xtrain <- split$train
Xtest <- split$test
```

We run the same analysis steps as above, but we run the clustering on ``Xtrain`` and we use ``Xtest`` as the response in our regression. 

```{r}
clusters.train <- kmeans(log(Xtrain+1), centers=2)$cluster
results.countsplit <- t(apply(Xtest, 2, function(u) summary(glm(u~clusters.train, family="poisson"))$coefficients[2,]))
head(results.countsplit)
```

We can already see from the summary output that the p-values for the first 6 genes are much larger. When we make the same uniform QQ-plot as before, we see that the p-values obtained from count splitting are uniformly distributed, as they should be under this global null where no genes are differentially expressed because all cells have expression counts drawn from the same distribution. 


```{r}
ggplot(data=NULL, aes(sample=results.countsplit[,4]))+geom_qq(distribution="qunif")+geom_abline(col="red")
```

## Simulated data with true signal

We now demonstrate the performance of count splitting on a simple simulated dataset that contains two true clusters. We first randomly assign the cells to one of two true clusters. We then generate data such that $X_{ij} \sim \mathrm{Poisson}(\Lambda_{ij})$. Genes $1--10$ are differentially expressed-- for $j=1,\ldots,10$, $\Lambda_{ij} = 5$ for cells in cluster $0$ and $\Lambda_{ij}=10$ for cells in cluster $1$. Genes $11--200$ are not differentially expressed ($\Lambda_{ij}=5$ for all cells). 

```{r}
set.seed(1)
n <- 1000
p <- 200
clusters.true <- rbinom(n, size=1, prob=0.5)
Lambda <- matrix(5, nrow=n, ncol=p)
Lambda[clusters.true==1, 1:10] <- 10
X <-apply(Lambda,1:2,rpois,n=1)
```

We now count split the data and save `Xtrain` and `Xtest` for later use.

```{r}
set.seed(111)
split <- countsplit(X, epsilon=0.5)
Xtrain <- split$train
Xtest <- split$test
```

First, let's look at the effect of count splitting on our ability to estimate the true clusters. 
If we use all of our data `X` to estimate the clusters, we make only 5 errors. 

```{r}
set.seed(222)
clusters.full <- kmeans(log(X+1), centers=2)$cluster
table(clusters.true, clusters.full)
```

If instead we use only `Xtrain` to estimate the clusters, we see that we make a few additional errors, but that in general we still get excellent estimates.

```{r}
clusters.train <- kmeans(log(Xtrain+1), centers=2)$cluster
table(clusters.train, clusters.full)
```

We now compare the slopes and intercepts that we estimate via count splitting to those that we would get in the ideal setting where we were able to observe the true clusters. 

```{r}
intercepts.ideal <- t(apply(X, 2, function(u) summary(glm(u~clusters.true, family="poisson"))$coefficients[1,1]))
slopes.ideal <- t(apply(X, 2, function(u) summary(glm(u~clusters.true, family="poisson"))$coefficients[2,1]))
```

Let's compare these ideal intercepts and slopes to the values that we get if we instead use either $\widehat{L}(X)$ or $\widehat{L}(X^\mathrm{train})$. 

```{r}
clusters.train <- kmeans(log(Xtrain+1), centers=2)$cluster
intercepts.countsplit <- t(apply(Xtest, 2, function(u) summary(glm(u~clusters.train, family="poisson"))$coefficients[1,1]))
slopes.countsplit <- t(apply(Xtest, 2, function(u) summary(glm(u~clusters.train, family="poisson"))$coefficients[2,1]))
```

This plot shows what we get.
```{r}
library(ggplot2)
library(patchwork)
p1 <- ggplot(data=NULL, aes(x=intercepts.ideal, y=intercepts.countsplit))+geom_point()+geom_abline(intercept= log(0.5), slope=1, col="red")
p2 <- ggplot(data=NULL, aes(x=slopes.ideal, y=slopes.countsplit))+geom_point()+geom_abline(intercept=0, slope=1, col="red")
p1 + p2
```


We see that